{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import neural_net_helper\n",
    "%aimport neural_net_helper\n",
    "\n",
    "nnh = neural_net_helper.NN_Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Re-using Pre-trained models\n",
    "\n",
    "\n",
    "In the module on Generative AI using an RNN, we [linked](https://app.inferkit.com/demo) to a model that generates text that continues an initial \"seed\" sequence of words.\n",
    "\n",
    "As we saw: the stories it produced were impressive.\n",
    "\n",
    "This, and similar models, were trained on a simple task that we called \"Predict the Next\"\n",
    "- given an initial sequence (e.g., of word)\n",
    "- predict the next element of the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We already know enough to be able to construct such a model ourselves:\n",
    "- An initial network (e.g., LSTM, Transformer) that transforms a sequence into a fixed length representation\n",
    "- Followed by a Classifier head (to generate a probability distribution over possible next elements)\n",
    "<br>\n",
    "<table>\n",
    "<tr>\n",
    "    <th><center><strong>RNN Many to one; followed by classifier</strong></center></th>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><img src=\"images/RNN_many_to_one_to_classifier.jpg\" width=870%></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We even demonstrated how to construct a training example for this task\n",
    "\n",
    "For example, given a sequence of words\n",
    "\n",
    "$\\mathbf{s} = $\n",
    "\"I am taking a class in Machine Learning\"\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{array}\\\\\n",
    "i & \\x^\\ip  & \\y^\\ip \\\\\n",
    "1 & [\\;  \\text{I} \\; ] & \\text{am} \\\\\n",
    "2 & [\\; \\text{I, am} \\; ] & \\text{taking} \\\\\n",
    "3 & [\\; \\text{I, am, taking} \\; ] & \\text{a} \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Simple enough.\n",
    "\n",
    "But one reason the linked model was so powerful was because of its size and training\n",
    "- 3 billion weights !\n",
    "- Trained on 500 billion tokens\n",
    "    - Used \\\\$ 42K of electricity\n",
    "    \n",
    "Even if this is not beyond our intellectual capacity, training such a model may be beyond our physical and financial capacities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The good news is that researchers often publish their models and trained weights in a public repo.\n",
    "\n",
    "We can re-use their models for free !\n",
    "- In our DL Assignments, you were asked to save your models and trained weights in a file\n",
    "- You can share your models too !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## [HuggingFace](https://huggingface.co/models)\n",
    "- Keras/PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Keras project Pre-trained Models\n",
    "\n",
    "### Image\n",
    "[ImageNet pre-trained models](https://keras.io/applications/)\n",
    "\n",
    "### NLP\n",
    "[Pre-trained word embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model zoo\n",
    "[Open source, pre-trained models](https://modelzoo.co)\n",
    "- [YOLO: object detection and classification](https://modelzoo.co/model/yolo-tensorflow)\n",
    "- [OpenPose: Human body, hand and facial keypoints](https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/.github/media/pose_face_hands.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer Learning: how to learn from little data\n",
    "\n",
    "Using pre-trained models is great if our task is identical to the training task and our test examples are from an *identical* distribution as the training examples.\n",
    "\n",
    "But it is often the case that even if the task is identical, we want to use test examples from a *similar* distribution.\n",
    "\n",
    "Consider how the mean of words change in the specialized domain of Quant Finance compared to General English\n",
    "\n",
    "Word            | English meaning | Quant Finance meaning\n",
    ":--|:--|:---\n",
    "call (noun) | cry; contact | type of option\n",
    "bull        | male bovine  | optimist\n",
    "\n",
    "A \"predict the next\" word task trained on General English is likely to perform below expectations for examples from Quant Finance.\n",
    "\n",
    "It would seem that a Quant Finance researcher would need to train on specialized examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The biggest constraint in training a model is obtaining a sufficient amount of training data.\n",
    "\n",
    "The deeper (greater number of layers) your model\n",
    "- The more weights/parameters need to be estimated\n",
    "- Which increases the need for a larger quantity of training data\n",
    "\n",
    "Obtaining large quantities of examples from specialized domains is challenging, labor-intensive and time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Enter *Transfer Learning*\n",
    "- Import the architecture and weights of a Pre-Trained model from the Source Task\n",
    "    - trained with a large quantity of general examples\n",
    "- \"Fine tune\" the weights to adapt the model to solve the Target Task\n",
    "    - by *continuing the training* on a *small* quantity of specialized examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall our module on Interpreting the layers of a Neural Network (NN)\n",
    "- layers close to the input seem to learn simple features\n",
    "- layer $\\ll$ creates new features that are combinations of features of layer $(\\ll-1)$\n",
    "\n",
    "Is it possible that we can \"re-use\" feature transformations ?\n",
    "- Use the layers closest to input for a Neural Network trained on a \"source\" Task\n",
    "- But apply these layers (and their transformations on input) to a new \"target\" Task ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Yes !  \n",
    "\n",
    "*Transfer Learning* creates a Neural Network for the new Target Task by\n",
    "- Using some number of layers (closest to input) of a *pre-trained* model for some Source Task\n",
    "- Appending new *untrained* layers for the Target Task\n",
    "    - e.g., final \"head\": regression, classification\n",
    "- Learning weights for the new *untrained* layers by training on examples from the Target Task\n",
    "    - Possibly modifying weights on the layers imported from the Source Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hopefully a picture will clarify.\n",
    "\n",
    "Here is the multi-layer network for the Source Task\n",
    "- The Classifier \"head\" is specialized to the Source Task (e.g., the classes of the Source task)\n",
    "- For example: recognizing images from 1000 possible classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transfer Learning: pre-trained model</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transfer_Learning_1.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And here\n",
    "- We change the inputs from $\\x$ (Source Task) to $\\x'$ (Target Task)\n",
    "- Replacing the Classifier for the Source Task\n",
    "- With a Classifier specialized to the Target Task (e.g., the classes of the Target task)\n",
    "- For example: recognizing images from 2 new classes distinct from the Source task's "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transfer Learning: replace the head of the pre-trained model</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transfer_Learning_2.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We **import** all the weights from the Source Task layers\n",
    "- We **retrain** the new Target Task Classifier Head using the training data for the Target Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Transfer Learning\n",
    "- Leverages all the effort in training the Source Task\n",
    "- To benefit training the Target Task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This technique is most valuable when the model for the Source task has been trained\n",
    "- With a **large** quantity of data\n",
    "- For a **long** time\n",
    "\n",
    "and when The Target task\n",
    "- Has a **small** quantity of training data\n",
    "- Limited computational resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why might this work ?\n",
    "- The weights of the Source Task\n",
    "- Have been laboriously trained\n",
    "- To recognize \"concepts\"\n",
    "- That transfer to other Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some examples may help\n",
    "\n",
    "**Image Classification**\n",
    "\n",
    "- Source Task: Label images to one of a 1000 possible classes (ImageNet)\n",
    "- Target Task: Label images with classes **distinct** from the Source classes\n",
    "\n",
    "Target task may have a very limited training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Self driving cars**\n",
    "- Source Task: Frames from a video game driving simulator\n",
    "- Target Task: Driving a real car \n",
    "\n",
    "Real training data (actual driving) hard to obtain\n",
    "- Simulation is easy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training the Target task\n",
    "\n",
    "When we import the Source Task's layers\n",
    "- We retain (**freeze**) their weights.\n",
    "- While we train/learn weights for the newly appended suffix of Target Task layers (e.g., the new head)\n",
    "\n",
    "Why is this a good idea ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The newly appended layers for the Target Task\n",
    "- Have **uninitialized** weights\n",
    "- Which means that the gradients for the Loss function will likely  be large at first\n",
    "\n",
    "If we **did not** freeze the imported weights\n",
    "- The imported layers would likely \"forget\" the concepts that were learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once the suffix has been sufficiently trained so that it's weights are no longer random\n",
    "- We *may* unfreeze weights of the imported layers\n",
    "- That are *closest to the new head*\n",
    "- Using a *much lower* learning rate\n",
    "\n",
    "In other words: we try to \"fine-tune\" the prefix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Fine tuning may allow the imported layers to adapt to the Target task.\n",
    "\n",
    "The key is to remember that\n",
    "- The imported layers have been trained on **many** examples\n",
    "- So their weights are less likely to need to adapt\n",
    "- Than the weights of the suffix, which have been trained on **few** examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sometimes the learning rate is varied by layer\n",
    "- Imported layers have low learning rates\n",
    "- Suffix layers have higher learning rates\n",
    "- Degenerate case: learning rate for imported layers is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "There is no guarantee that the features learned by the Source Task will be useful for the Target task.\n",
    "\n",
    "The chances may improve if the domains of the Source and Target tasks are more similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer learning in Keras\n",
    "\n",
    "From a coding perspective, Transfer Learning looks something like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "target_model = Sequential()\n",
    "\n",
    "# Import the prefix of source_model\n",
    "# - Import the architecture\n",
    "# - and the weights\n",
    "# Freeze the imported weights\n",
    "for layer in source_model.layer[:num_prefix_layers]:\n",
    "    target_model.add( layer, trainable=False )\n",
    "    \n",
    "# Add the Suffix of the target task to the model\n",
    "target_model.add( ... )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Keras Sequential model is similar to an array of layers\n",
    "- We can index into the array to obtain slices (the prefix)\n",
    "- We can append to the array to add the new suffix\n",
    "\n",
    "We will show some real code shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to choose the prefix of the Source task\n",
    "\n",
    "It is not necessary (nor advisable) to keep **all** the layers of the Source Task's model\n",
    "\n",
    "A smaller prefix might be better.\n",
    "\n",
    "But where should we truncate the Source task's model ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If the prefix is too small\n",
    "- That is: we retain only the shallow layers of the Source Task's model\n",
    "- We may not benefit from the fullness of the concepts learned by the Source Task\n",
    "    - Similar to underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On the other hand, if the prefix is too large\n",
    "- That is: we retain very deep layers of the Source Task's model\n",
    "- We may have concepts that are so *specialized* for the Source Task\n",
    "- That they fail to benefit the Target Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The best advice: experiment !\n",
    "\n",
    "We will do exactly that in the example notebook for this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transfer Learning: replace the head, deep layers of the pre-trained model</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transfer_Learning_3.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Transfer learning is a method to make you highly productive\n",
    "- Leverage an existing model that may have been very expensive to train\n",
    "    - Revolutionized Image Processing and Natural Language Processing\n",
    "- \"Cut off the head\" and retrain *new head* on smaller number of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But there is still an element of art in knowing how much of the head to cut off\n",
    "- Deeper layers may have over-specialized; best to cut them off\n",
    "- Shallower layers may only recognize generic features; best to keep more of them\n",
    "\n",
    "The Keras Guide to [Transfer Learning & Fine-Tuning](https://keras.io/guides/transfer_learning/) is a good reference for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
