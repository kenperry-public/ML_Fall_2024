{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{p}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actionvalfun}{q}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dense representation for categorical variables\n",
    "\n",
    "One drawback that we flagged with One Hot Encoding of a categorical variable\n",
    "- Tokens that seem to be related in the input domain (\"dog\", \"dogs\")\n",
    "- Become *unrelated* when One Hot Encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Because each value is a long vector\n",
    "- With a single non-zero element\n",
    "- That is *different* across the two values\n",
    "- The OHE vectors are *orthogonal*\n",
    "\n",
    "This means that there is no useful measure of the distance between two tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To illustrate the \"lack of distance\" issue, let $\\text{rep}$ be a mapping from tokens to their One Hot Encodings.\n",
    "\n",
    "Using dot product (cosine similarity) as a measure of similarity to the token \"dog\"\n",
    "\n",
    "| word   | rep(word) | Similarity to \"dog\"|\n",
    "| ---    | ---       | :---:        |\n",
    "| dog   | [1,0,0,0]   | rep(word) $\\cdot$ rep(dog)  = 1  |\n",
    "| dogs  | [0,1,0,0]   | rep(word) $\\cdot$ rep(dog)  = 0  |\n",
    "| cat   | [0,0,1,0]   | rep(word) $\\cdot$ rep(dog)  = 0  |\n",
    "| apple | [0,0,0,1]   | rep(word) $\\cdot$ rep(dog)  = 0  |\n",
    "\n",
    "All words other than \"dog\" are equidistant from \"dog\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Intuitively, we observe similarity between \"dog\" and other words\n",
    "- (\"dog\", \"dogs\"): Same root, different Singular/Plural form \n",
    "- (\"dog\", \"cat\"); Same concept: pet\n",
    "\n",
    "and complete lack of similarity between \"dog\" and \"apple\".\n",
    "\n",
    "Yet all have the same distance measure from \"dog\": $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can consider an alternate encoding to OHE.\n",
    "\n",
    "Suppose each dimension of the encoded vector\n",
    "- Measured the intensity of the token against some concept\n",
    "    - Singular/Plural\n",
    "    - Domestic Animal\n",
    "    - Edible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This type of representation is called *continuous*\n",
    "- As the strength is a continuous value\n",
    "- Compared to the *discrete* encoding of OHE as binary 0/1\n",
    "\n",
    "It is also called a *dense* representation\n",
    "- Multiple non-zero elements in the vector\n",
    "- Compared to the single non-zero element in the OHE vector\n",
    "\n",
    "In a continuous, dense representation\n",
    "two values expressing similar concepts will be \"closer\" than two values that do not share concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- \"Cats\", \"Dogs\", \"Apples\"\n",
    "    - Share the concept \"Plural\"\n",
    "- \"Cat\", \"Dog\"\n",
    "    - Share the concept \"Domestic animal\"\n",
    "- \"good\", \"bad\"\n",
    "    - Share the concept \"Opposite\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Doing math with words\n",
    "\n",
    "Let's explore the implication and power of dense vector representation of words.\n",
    "\n",
    "If each element of the vector\n",
    "- Expresses a concept\n",
    "- And the number of concepts is small compared to $|| \\Vocab ||$\n",
    "- And the concepts are fairly independent\n",
    "- Then we have found an alternate basis (compared to the $|| \\Vocab ||$ basis vectors of OHE) of smaller dimension\n",
    "- For representing words\n",
    "\n",
    "This concept is sometimes called *word embeddings*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $\\v_w$ denote the dense representation of token $w$:\n",
    "\n",
    "\n",
    "| $w$   | $\\v_w$ |\n",
    "| ---    | ---       | \n",
    "| cat   | [.7, .5, .01 ]   \n",
    "| cats   | [.7, .5, .95 ]  \n",
    "| dog   | [.7, .2, .01 ]   \n",
    "| dogs   | [.7, .2, .95 ]\n",
    "| apple   | [.1, .4, .01 ]   \n",
    "| apples   | [.1, .4, .95 ]\n",
    "\n",
    "Notice that \"dogs\" and \"apples\"\n",
    "- Are similar along one dimension (the last, perhaps encoding \"Is Plural\")\n",
    "- Are dissimilar along one dimension (the first, perhaps encoding \"Is Pet\")\n",
    "\n",
    "Also notice that \"dog\" and \"cat\"\n",
    "- Are similar along the first dimension (reinforcing the notion that this dimension may be \"Is Pet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Taking this a step further: we can perform element-wise math on dense vector representations:\n",
    "\n",
    "$$\n",
    "\\v_\\text{cats} - \\v_\\text{cat} \\approx \\v_\\text{dogs} - \\v_\\text{dog} \\approx \\v_\\text{apples} - \\v_\\text{apple}\n",
    "$$\n",
    "\n",
    "because\n",
    "- \"cats\" and \"cat\" are similar in all concepts *except* \"Plural\".\n",
    "- As are \"dogs\" and \"dog\"\n",
    "- As are \"apples\" and \"apple\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If that's the case, we can approximate the vector that expresses the \"pure\" concept \"Is Plural\"\n",
    "- Without expressing any other concept\n",
    "\n",
    "as $(\\v_\\text{cats} - \\v_\\text{cat})$\n",
    "\n",
    "Then we can construct the Plural form of \"apple\"\n",
    "- By adding the pure vector for Plural to the vector for \"apple\"\n",
    "\n",
    "$$\n",
    "\\v_\\text{apples} \\approx \\v_\\text{apple} + (\\v_\\text{cats} - \\v_\\text{cat})\n",
    "$$\n",
    "we can create the Plural form of \"apple\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word analogies\n",
    "\n",
    "The implications of doing math on words is even more powerful.\n",
    "\n",
    "Consider solving the analogy problem\n",
    ">king:man :: ?:woman\n",
    "\n",
    "That is: what is the female analog of \"king\" ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose the concepts (\"dimensions\") of the dense representation were\n",
    "- Gender (man or woman)\n",
    "- Regal (Royal or commoner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then\n",
    "$$\n",
    "\\v_\\text{king} - \\v_\\text{man} + \\v_\\text{woman} \\approx \\v_\\text{queen}\n",
    "$$\n",
    "because\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\v_\\text{king} & = & (\\text{Man, Royal}) & \\text{vector representation of \"king\"}\\\\\n",
    "\\v_\\text{king}- (\\text{Man}, 0) &  = & (0, \\text{Royal}) & \\text{subtract vector for \"pure\" concept \"Man\"} \\\\\n",
    "\\v_\\text{king} - (\\text{Man}, 0) + (\\text{Woman}, 0) & = & (\\text{Woman, Royal})  & \\text{add vector for \"pure\" concept \"Woman\"} \\\\\\\\\n",
    "& = & \\text{Queen} & \\text{the word having concepts \"Woman\" and \"Royal\"}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We can use math on dense vectors to compute analogies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's formalize the \"math\" of word vectors\n",
    "\n",
    "For tokens $w, w'$ with dense vectors $\\v_{w}, \\v_{w'}$\n",
    "- Define a metric $d(\\v_{w}, \\v_{w'})$ of the distance between the words\n",
    "- For example: \n",
    "$$d(\\v_{w}, \\v_{w'}) = 1 - \\text{cosine similarity}(\\v_{w}, \\v_{w'})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Define the set of tokens $N_{n',d}(w)$ in vocabulary $\\Vocab$\n",
    "- That are among the $n'$ \"closest\" to a token $w$\n",
    "- According to distance metric $d$\n",
    "\n",
    "$$\n",
    "\\begin{array}\\\\\n",
    "\\text{wv}_{n',d}(w) & = & \\left\\{ \\; \\v_{w'} \\, | \\, \\text{rank}_V( d(\\v_{w}, \\v_{w'}) ) \\le n' \\; \\right\\} & \n",
    "\\text{the dense vectors of the } n' \\text{ tokens in } \\Vocab \\text{ closest to token } w \\\\\n",
    "N_{n',d}(w) & = & \\left\\{ \\; w' \\, | \\, \\v_{w'} \\in \\text{wv}_{n',d}(w) \\; \\right\\} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "This is the \"neighborhood\" of token $w$ as defined by the distance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Token $w'$ is defined to be *approximately equal to* token $w$ \n",
    "- Denoted as $w \\approx_{n',d} w'$\n",
    "- If $w'$ is in the neighborhood of $w$\n",
    "\n",
    "$$\n",
    "w \\approx_{n',d} w' \\; \\; \\text{if } \\w' \\in N_{n',d}(w) \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, the analogy\n",
    ">a:b :: c:d\n",
    "\n",
    "implies\n",
    "\n",
    "$$\n",
    "\\v_a - \\v_b  \\approx_{n',d}  \\v_c - \\v_d \n",
    "$$\n",
    "\n",
    "So to solve the word analogy for $c$:\n",
    "$$\n",
    "\\v_c \\approx_{n',d}  \\v_a - \\v_b + \\v_d\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GloVe: Pretrained embeddings\n",
    "\n",
    "Fortunately, you don't have to create your own word-embeddings from scratch.\n",
    "\n",
    "There are a number of precomputed embeddings freely available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "GloVe is a family of word embeddings that have been trained on large corpora\n",
    "- GloVe6b\n",
    "    - Trained on 6 Billion tokens\n",
    "    - 400K words\n",
    "    - Corpus:  Wikipedia (2014) + GigaWord5 (version 5, news wires 1994-2010)\n",
    "    - Many different dense vector lengths to choose from\n",
    "        - 50, 100, 200, 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will illustrate the power of word embeddings using GloVe6b vectors of length $100$.\n",
    "\n",
    "$\n",
    "\\begin{array}[llllll]\\\\\n",
    "\\text{king- man + woman} &  \\approx_{n',d} & \\text{queen } \\\\\n",
    "\\text{man - boy + girl} &  \\approx_{n',d} & \\text{woman } \\\\\n",
    "\\text{Paris - France + Germany} &  \\approx_{n',d} & \\text{Berlin } \\\\\n",
    "\\text{Einstein - science + art} &  \\approx_{n',d} & \\text{Picasso} \\\\\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "You can see that the dense vectors seem to encode \"concepts\", that we can manipulate mathematically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You may discover some unintended bias\n",
    "\n",
    "$\n",
    "\\begin{array}[llllll]\\\\\n",
    "\\text{doctor - man + woman} &  \\approx_{n',d} & \\text{nurse } \\\\\n",
    "\\text{mechanic  - man + woman} &  \\approx_{n',d} & \\text{teacher } \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Domain specific embeddings\n",
    "\n",
    "Do we speak Wikipedia English in this room ?\n",
    "\n",
    "Here are the neighborhoods of some financial terms, according to GloVe:\n",
    "\n",
    "$\n",
    "\\begin{array}[lll]\\\\\n",
    "N(\\text{bull}) & =  & [ \\text{cow, elephant, dog, wolf, pit, bear, rider, lion, horse}] \\\\\n",
    "N(\\text{short}) & =  & [ \\text{rather, instead, making, time, though, well, longer, shorter, long}] \\\\\n",
    "N(\\text{strike}) & =  & [ \\text{workers, struck, action, blow, striking, protest, stoppage, walkout, strikes}] \\\\\n",
    "N(\\text{FX}) & =  & [ \\text{showtime, cnbc, ff, nickelodeon, hbo, wb, cw, vh1}] \\\\\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "It may be desirable to create word embeddings on a narrow (domain specific) corpus.\n",
    "\n",
    "This is not difficult provided you have enough data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Obtaining Dense Vectors: Transfer Learning\n",
    "\n",
    "How do we obtain Dense Vector representations that seem to have these wonderful properties ?\n",
    "\n",
    "- Through Machine Learning !\n",
    "- As a by-product of solving a specific Source task\n",
    "- Once we have the embeddings, we can re-use them in many other Target tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is exactly what we called Transfer Learning\n",
    "- We train a Source Task\n",
    "- The layers and associated weights learned in training the Source Task\n",
    "- Are re-used for a different Target Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The layer and associated weights that implement the dense vector encoding are re-used\n",
    "- Called an *Embedding Layer*\n",
    "\n",
    "We will show code that trains an Embedding Layer shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word prediction problems: high-level\n",
    "\n",
    "The Source Task we will use to create word embeddings is from a class of *Word Prediction* tasks\n",
    "- Given a set of tokens (the \"context\")\n",
    "- Predict a related token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example\n",
    "- Given prefix $\\w_{(1)} \\ldots \\w_{(\\tt-1)}$ of a sequence of tokens $\\w$\n",
    "- Predict the next token $\\w_\\tp$.\n",
    ">\"Machine Learning is  $\\langle ??? \\rangle $\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or a similar problem\n",
    "- Predict token $\\w_\\tp$\n",
    "- From surrounding tokens \n",
    "$$\\w_{(\\tt-o)}, \\ldots, \\w_{(\\tt-1)} , \\langle ??? \\rangle, \\w_{(\\tt+1)} \\ldots, \\w_{(\\tt+o)}$$\n",
    ">\"Machine  $\\langle ??? \\rangle$ is easy\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The inspiration behind using a Word Prediction task to learn embeddings\n",
    "- Is that meaning of a word can be inferred by context\n",
    "- \"You are known by the company that you keep\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example\n",
    "- \"I ate an apple\"\n",
    "- \"I ate a  blueberry\"\n",
    "- \"I ate a pie\"\n",
    "\n",
    "\"apple\", \"blueberry\", \"pie\" concept: things that you eat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Word Prediction task is thus a form of Classification.\n",
    "\n",
    "We need a large number of training examples as this is a Supervised Learning problem.\n",
    "\n",
    "One reason that Word Prediction is used is that it is fairly easy to obtain training examples\n",
    "- From any source of raw text\n",
    "- Just reformat\n",
    "- That is: the target/label for an example is just an adjacent token\n",
    "\n",
    "Since targets can be derived from examples, this is sometimes called *Semi-Supervised Learning*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Let $\\w$ be the sequence of $n_\\w$ words \n",
    "\n",
    "A *word prediction* is a mapping \n",
    "- from input $\\w$\n",
    "- to a probability distribution $\\hat{\\y}$ over all words in vocabulary $\\Vocab$\n",
    "    - $\\hat{\\y}_j = \\pr{V_j}$\n",
    "    - That is: it assigns a probability to each word in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here are some simple word prediction problems:\n",
    "\n",
    "$\n",
    "\\begin{array}[lll]\\\n",
    "\\text{predict next word from context}  & \\pr{\\w_\\tp | & \\w_{(\\tt-\\offset)} \\ldots, \\w_{(\\tt-1)} } \\\\\n",
    "\\text{predict a surrounding word}      & \\pr{\\w_{(\\tt')} |& \\w_\\tp } \\\\\n",
    "    & & \\tt' = \\{ \\tt - o, \\ldots, \\tt + o \\} - \\{ \\tt \\} \\\\\n",
    "\\text{predict center word from context} & \\pr{ \\w_\\tp | & [ \\w_{(\\tt-\\offset)} \\ldots \\w_{(\\tt-1)} \\w_{(\\tt+1)} \\ldots \\w_{(\\tt+\\offset)} ] }  & \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is the Neural Network we construct for the Source Task that will learn embeddings\n",
    "- Ignoring for the moment the issue of converting variable length sequences to a fixed length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Embedding Layer</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Embedding_Layer.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Layers:\n",
    "- One Hot Encoded token\n",
    "- Embedding: converts sparse encoding to dense encoding\n",
    "- Classifier: operating on dense encodings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The only \"new\" layer type is the Embedding layer\n",
    "- This is nothing more than Matrix Multiplication\n",
    "- The mapping can be implemented  an $(|\\Vocab| \\times n_e)$ matrix\n",
    "$\\Emb$ \n",
    "- Where $n_e$ is the length chosen for the dense vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is because \n",
    "- The OHE vector for the $j^{th}$ word $\\Vocab_j$ in vocabulary $\\Vocab$\n",
    "- Is the $(|\\Vocab| \\times 1)$ vector of all $0$'s except at index $j$\n",
    "$$\n",
    "V^{(j)} =1\n",
    "$$\n",
    "- $\\Emb^T * \\text{OHE}(\\Vocab_j)$\n",
    "- Selects row $j$ of $\\Emb$, which is the  is the $(n_e \\times 1)$ *dense vector* encoding of $\\Vocab_j$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Matrix $\\Emb$ are *weights to be learned* by training\n",
    "- Along with the weights of the Classifier layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In other words\n",
    "- We train the Neural Network\n",
    "- To create an embedding\n",
    "- That makes it easy for a Classifier\n",
    "- To solve the Source Task\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Categorical variables (such as tokens/words) are easily represented as One Hot Encoded values.\n",
    "\n",
    "This is perfectly adequate when there is no relationship between tokens.\n",
    "\n",
    "Word embeddings/Dense representations create a representation\n",
    "- Not just of a token is isolation\n",
    "- But a token with multiple dimensions of meaning\n",
    "- Which enable inter-token relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We showed how to create dense representation of words as a by-product of solving a Source Task.\n",
    "\n",
    "The Source Task we used was Word Prediction, but other tasks may work as well.\n",
    "\n",
    "The embeddings learned for the Source Task may be useful in other tasks\n",
    "- This is Transfer Learning in the real world\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.562px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
