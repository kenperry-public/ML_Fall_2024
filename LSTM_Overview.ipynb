{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The LSTM API\n",
    "\n",
    "A vanilla RNN uses the latent state vector $\\h_\\tp$ to assist with \n",
    "- Determining the next output $\\y_\\tp$ (for a many to one RNN)\n",
    "- Updating the latent state from $\\h_\\tp$ to $\\h_{(\\tt+1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An LSTM separates these two tasks, using\n",
    "- $\\h_\\tp$ as \"short-term\" memory (control state)\n",
    "- For controlling state transition from $\\h_\\tp$ to $\\h_{(\\tt+1)}$\n",
    "\n",
    "and an additional vector\n",
    "- $\\c_\\tp$ as a \"long-term\" memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some analogies for the differing roles of short and long term memory\n",
    "- Your computer\n",
    "    - Short term memory: RAM (Random Access Memory)\n",
    "    - Long term memory: a disk, memory stick, flash card\n",
    "- Your office\n",
    "    - Short term memory: the desktop\n",
    "    - Long term memory: the filing cabinet\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's introduce [the basic elements of an LSTM](LSTM_API.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inside an LSTM Layer\n",
    "\n",
    "Time to explore the inner workings on an LSTM.\n",
    "- It will seem complicated at first\n",
    "- Many small, inter-connected pieces\n",
    "- Lots of literature with confusing diagrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a typical diagram from the literature that tries to explain an LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>LSTM diagram</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/3/3b/The_LSTM_cell.png\" width=600></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Attribution: https://commons.wikimedia.org/wiki/File:The_LSTM_cell.png</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will try to manage the complexity\n",
    "- Following the \"classical\" derivation of the [original paper that introduced the LSTM](https://www.bioinf.jku.at/publications/older/2604.pdf)\n",
    "- But influenced by an excellent [blog post](http://blog.echen.me/2017/05/30/exploring-lstms/)\n",
    "\n",
    "Let's [go inside an LSTM](LSTM_Workings.ipynb) to see how this happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  LSTM as gated residual connections\n",
    "\n",
    "How does an LSTM circumvent the potential problem of vanishing and exploding gradients ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that a very deep NN (such as an \"unrolled\" RNN on a long sequence)\n",
    "- is exposed to the problem of Vanishing/Exploding gradients\n",
    "- because the Loss Gradient gets moderated by\n",
    "$$\n",
    "\\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)} }\n",
    "$$\n",
    "during Back Propagation\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll]\\\\\n",
    "\\loss'_{(\\ll-1)} & = &  \\loss'_\\llp \\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}} \\\\\n",
    "         & = &  \\loss'_\\llp a'_\\llp f'_\\llp\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- where $\\y_\\llp$ computes\n",
    "$$\n",
    "a_\\llp ( f_\\llp(\\y_{(\\ll-1)}, \\W_\\llp))\n",
    "$$\n",
    "- and $a' \\lt 1$ for many activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Loss: Backward pass</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_loss_gradient.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But the LSTM has the ability\n",
    "- through certain settings of the gates\n",
    "- to have $\\y_\\llp$ compute\n",
    "- the identity function\n",
    "$$\n",
    "\\y_\\llp = \\y_{(\\ll-1)}\n",
    "$$\n",
    "\n",
    "When this happens\n",
    "$$\n",
    "\\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)} } = 1\n",
    "$$\n",
    "and the Loss Gradient flows backward undiminished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Examine the update equation for the long term memory of an LSTM:\n",
    "\n",
    "$$\\c_\\tp = \\remember_\\tp \\otimes \\c_{(t-1)} + \\save_\\tp \\otimes \\c'_\\tp$$\n",
    "\n",
    "and consider the case when\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\remember_{\\tp,j} & = & 1 & \\text{Allow prior value of } j^{th} \\text{ element to be updated} \\\\\n",
    "\\save_{\\tp, j} & = & 0 & \\text{But don't allow the candidate update value } \\c'_{\\tp,j} \\text{ to participate}\n",
    "\\end{array}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then\n",
    "$$\n",
    "\\c_\\tp = \\c_{(\\tt-1)}\n",
    "$$\n",
    "\n",
    "and the \"circuit\" for this element becomes a skip connection\n",
    "- Passing through prior value of element $j$ unchanged\n",
    "- No moderation of the Loss Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By \n",
    "- Having the ability to turn a layer into the identity transformation\n",
    "- Implementing a \"skip connection\" of a residual network\n",
    "\n",
    "the LSTM can avoid the problem of Vanishing/Exploding gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If $\\focus_{\\tp,j}$ is also equal to $1$\n",
    "\n",
    "$$\\h_\\tp = \\focus_t \\otimes \\tanh(\\c_\\tp)$$\n",
    "is also unchanged.\n",
    "\n",
    "So, if the Loss function would be minimized by an identity transformation\n",
    "- The LSTM is able to implement this transformation\n",
    "- By the appropriate choice of weights $\\W$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Initial bias to \"not forget\"\n",
    "\n",
    "A very deep multi-layer network may have trouble learning in early epochs of training\n",
    "- Uninitialized weights in deep layers\n",
    "- No specialization of shallow layers to uncover features that would be useful to the deeper layers\n",
    "- Resulting in large Loss function values\n",
    "- Large Loss Gradients make weight updates unstable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The ability of the LSTM to implement the identity transformation comes to the rescue !\n",
    "\n",
    "If we could force $\\remember_\\tp$ to $1$ early in training\n",
    "- We could get the LSTM to implement the identity transformation\n",
    "- And allow deep LSTM layers to not influence weight updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We simply set the bias $\\b_f$ of the $\\remember$ gate to a large positive value\n",
    "$$\\remember_\\tp   =  f_\\tp   =  \\sigma(\\W_{x,f} \\x_{(t)} + \\W_{h,f}\\h_{(t-1)} + \\b_f) $$\n",
    "which forces the sigmoid to output $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is *really* going on inside an LSTM\n",
    "\n",
    "The mechanics of the LSTM feel complicated.\n",
    "\n",
    "But let's not let that obscure what is going on inside the LSTM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's examine the update equation for the long-term memory $\\c_\\tp$\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\c'_\\tp  & = & \\tanh(\\W_{x,c} \\x_\\tp + \\W_{h,c}\\h_{(t-1)} + \\b_c) \\\\\n",
    "\\c_\\tp   & = & \\remember_\\tp \\otimes \\c_{(t-1)} + \\save_\\tp \\otimes \\c'_\\tp\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Note that the candidate update value $\\c'_\\tp$\n",
    "- Has been squashed by $\\tanh$ to the range $[-1, +1]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\remember_{\\tp,j} & = & 1 & \\text{Allow prior value of } j^{th} \\text{ element to be updated} \\\\\n",
    "\\save_{\\tp, j} & = & 1 & \\text{Allow the candidate update value } c'_{\\tp,j} \\text{ to participate}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "then the $j^{th}$ long-term memory element acts like a counter !\n",
    "- Incremented/Decremented by $\\c_\\tp \\in [-1, +1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our module on [RNN Visualization](RNN_Visualization.ipynb)\n",
    "- We speculated that the elements of the latent state $\\h_\\tp$\n",
    "- Of a vanilla RNN\n",
    "- Were implementing counters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>State activations after seeing prefix of input</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Unreasonable_effectiveness_cell2.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can imagine how a counter might be used in handling text input\n",
    "- As a switch indicating being inside/outside of delimiters like quotation marks\n",
    "- As a measure of how deeply nested an expression is (e.g., lists)\n",
    "- As a count of characters in a sentence (increasing probability of seeing an end-of-sentence delimiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The update equation for long-term memory\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\c'_\\tp  & = & \\tanh(\\W_{x,c} \\x_\\tp + \\W_{h,c}\\h_{(t-1)} + \\b_c) \\\\\n",
    "\\c_\\tp   & = & \\remember_\\tp \\otimes \\c_{(t-1)} + \\save_\\tp \\otimes \\c'_\\tp\n",
    "\\end{array}\n",
    "$$\n",
    "is an almost direct operational implementation of the counter concept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "We introduced an advanced Recurrent Layer type call the LSTM.\n",
    "\n",
    "Similar concepts are present in the Gated Recurrent Unit (GRU), another advanced Recurrent Layer type.\n",
    "\n",
    "These advanced concepts were designed with the specific intent of dealing with long sequences.\n",
    "\n",
    "They are thus quite common in domains such as Natural Language Processing, which has long-range dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
