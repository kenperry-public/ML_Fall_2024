{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "The datasets in the early days of Machine Learning were mainly numeric.\n",
    "\n",
    "This simplified an already difficult problem because numeric data is easily amenable to\n",
    "manipulation by a computer.\n",
    "\n",
    "But the quantity of non-numeric data, such as Image and Text, is increasing rapidly\n",
    "thanks to the omnipresence of Internet connected devices.\n",
    "\n",
    "*Natural Language Processing* is the set of techniques \n",
    "that facilitate using *unstructured* text as raw material.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What are some typical tasks involving NLP in Finance\n",
    "- Is a document *relevant* to a particular company or industry ?\n",
    "- Does the document express a Positive or Negative sentiment\n",
    "- Summarization of the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the NLP literature, there are a number of stylized tasks:\n",
    "- Language Model\n",
    "    - predict the next word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Entailment of two sentences\n",
    "    - Does the second sentence logically follow from the first ?\n",
    "    > \"Our class begins in the morning\" $\\leadsto$ \"The sun has not set at the start of class\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Reading comprehension\n",
    "    - Given a Context (e.g., article) and a question on its content, create an answer\n",
    "        >Context: Syllabus for our class\n",
    "\n",
    "        >Question: What is the topic for Week 12 ?\n",
    "\n",
    "        > Answer (free form): Most of the lecture is on NLP\n",
    "        \n",
    "        > Answer (multiple choice): \n",
    "        \n",
    "               (a) Classical ML (false);  (b) Regression (false)  (c) NLP (true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Similarity\n",
    "    - Given two sentences: do they express a similar thought ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "We will provide a brief introduction, based on Neural Network techniques.\n",
    "\n",
    "That is not to discount more \"classical\" methods for NLP\n",
    "- Part of speech tagging\n",
    "- Stemming\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "All of these are potentially useful as preprocessing steps for Deep Learning.\n",
    "\n",
    "However, if our data sets are big enough, it may be counter-productive to preprocess.\n",
    "- We may introduce our own biases\n",
    "- \"Let the data speak for itself\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Notation\n",
    "\n",
    "We define a finite set $\\Vocab$ to be the universe of possible tokens that make up text\n",
    "- We denote the items as *tokens* rather than *words*\n",
    "- Tokens may involve non-words such as punctuation (e.g., exclamation) and special characters (emoji)\n",
    "- Tokens may consist of sub-words \"un-important\"\n",
    "\n",
    "We denote the $j^{th}$ token in vocabulary $\\Vocab$ as $\\Vocab_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Text* $\\w$ is a *sequence* of tokens\n",
    "$$\n",
    "\\w_{(1)}, \\ldots, \\w_{(n_\\w)}\n",
    "$$\n",
    "using parenthesized subscripts to index into sequences, as usual.\n",
    "$$\n",
    "\\w_\\tp \\in \\Vocab \\text{ for all } 1 \\le \\tt \\le n_\\w\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Two non-word tokens are used to denote the start and end of the sequence\n",
    "of tokens in text.\n",
    "- $\\w_{(0)} = \\langle \\text{START} \\rangle$\n",
    "- $\\w_{(n_\\w +1)} = \\langle \\text{END} \\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is special about NLP ?\n",
    "\n",
    "We begin the study of NLP by discussing issues that are [particular to text](NLP_Tokens.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sentiment classification notebook on Colab: simple model\n",
    "\n",
    "Let's illustrate the traditional summarization of the sequence by some code\n",
    "for the following Classification task\n",
    "\n",
    "- Input: Movie review, as sequence of characters\n",
    "- Label: Positive/Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The first model will be extremely simple\n",
    "- Use One Hot Encodings to represent tokens\n",
    "- Use Global Pooling to reduce variable length sequences to fixed length\n",
    "- Followed by Binary Classifier\n",
    "\n",
    "<!--- #include (Keras_examples_imdb_cnn.ipynb)) --->\n",
    "\n",
    "[NLP notebook: examine the data](https://colab.research.google.com/github/kenperry-public/ML_Advanced_Fall_2024/blob/master/Keras_examples_imdb_cnn.ipynb#scrollTo=zpnaoupR9_QD)\n",
    "\n",
    "\n",
    "[NLP notebook: simple model](https://colab.research.google.com/github/kenperry-public/ML_Advanced_Fall_2024/blob/master/Keras_examples_imdb_cnn.ipynb#scrollTo=QtvUFJZJ7Oqi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "One Hot Encoding of tokens is the obvious choice, which is why we used it in our simple model.\n",
    "\n",
    "But we can do much better\n",
    "- Learn shorter encoding vectors\n",
    "- That also capture inter-token relationships\n",
    "\n",
    "We will study these short vectors in the notebook on\n",
    "[Embeddings](NLP_Embeddings.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning embeddings notebook Colab: Sentiment Classification\n",
    "\n",
    "Let's demonstrate\n",
    "- How we can learn embeddings\n",
    "- And use them to hopefully improve upon the simple model\n",
    "\n",
    "<!--- #include (Keras_examples_imdb_cnn.ipynb) --->\n",
    "\n",
    "[NLP notebook: learned embeddings](https://colab.research.google.com/github/kenperry-public/ML_Advanced_Fall_2024/blob/master/Keras_examples_imdb_cnn.ipynb#scrollTo=f5XrUD3X8KgP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using Convolutions for Text\n",
    "\n",
    "Our models still fail to take advantage of features particular to text.\n",
    "\n",
    "Because we used Global Pooling to summarize the sequence\n",
    "- We lost all information about token (word) order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We don't have to resort to the \"ultimate\" Layer-type for sequences (RNN) to take at least some advantage of order.\n",
    "\n",
    "A Convolutional Layer can capture some limited sense of the temporal ordering.\n",
    "\n",
    "Each kernel in the Convolution can\n",
    "- Create a synthetic feature\n",
    "- That is a combination of consecutive tokens\n",
    "- (What was a \"spatial\" dimension in, e.g., an image, is now a \"temporal\" dimension)\n",
    "\n",
    "Let's learn about the concept of\n",
    "[neural n-grams](NLP_Convolution.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Sentiment classification notebook on Colab : Convolutional n-grams\n",
    "\n",
    "Let's augment our Sentiment Classification notebook with neural n-grams.\n",
    "\n",
    "[NLP notebook: neural n-grams](https://colab.research.google.com/github/kenperry-public/ML_Advanced_Fall_2024/blob/master/Keras_examples_imdb_cnn.ipynb#scrollTo=LPChvdnzUY9f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Machine Learning on textual data is becoming an increasingly important source of insight in many domains.\n",
    "\n",
    "NLP brought together many elements of the knowledge we gained in this course\n",
    "- One Hot Encoding of tokens\n",
    "- Reducing variable length sequence inputs to fixed length (RNN)\n",
    "- Classical ML (Classification)\n",
    "\n",
    "So, as useful as this lecture was in its own right, we hope it reinforces everything you've learned throughout this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
