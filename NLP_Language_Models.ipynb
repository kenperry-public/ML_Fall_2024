{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Language Models\n",
    "\n",
    "A *Language Model* is an instance of the \"predict the next\" paradigm where\n",
    "- given a sequence of tokens\n",
    "- we try to predict the next token\n",
    "\n",
    "$$\n",
    "\\pr{\\y_\\tp \\, | \\, \\y_{(1:\\tt-1)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall the architecture to solve \"predict the next word\" and data preparation\n",
    "\n",
    "<br>\n",
    "<table>\n",
    "<tr>\n",
    "    <center><strong>Language Modeling task</strong></center>\n",
    "</tr>\n",
    "    <br>\n",
    "<tr>\n",
    "    <th><center>Architecture</center></th>\n",
    "    <th><center>Data preparation</center></th>\n",
    "    </tr>\n",
    "<tr>\n",
    "    <td><img src=\"images/RNN_many_to_one_to_classifier.jpg\" width=70%></td>\n",
    "    <td><center>$\\y = \\y_{(1)}, \\ldots, \\y_{(T)}$</center>\n",
    "        <br><br><br>\n",
    "\\begin{array} \\\\\n",
    "      i  & \\x^\\ip  & \\y^\\ip \\\\\n",
    "      \\hline \\\\\n",
    "      1 & \\y_{(0) }  & \\y_{(1)} \\\\\n",
    "      2 & \\y_{([0:1]) }  & \\y_{(2)} \\\\\n",
    "      \\vdots \\\\\n",
    "      \\tt & \\y_{([1:\\tt-1]) }  & \\y_\\tp \\\\\n",
    "      \\vdots \\\\\n",
    "      T & \\y_{([1:T-1]) }  & \\y_{(T)} \\\\\n",
    "\\end{array}\n",
    "    </td>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The raw data\n",
    "- e.g., the sequence of words $\\mathbf{s} = \\mathbf{s}_{(1)}, \\ldots \\mathbf{s}_{(\\bar T)}$\n",
    "\n",
    "is not naturally labeled.\n",
    "\n",
    "We need a Data Preparation step to create labeled example $i$\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\x^\\ip & = & \\mathbf{s}_{(1)}, \\ldots \\mathbf{s}_{(i)} \\\\\n",
    "\\y^\\ip & = & \\mathbf{s}_{(i+1)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We have called this method of turning unlabeled data into labeled examples: *Semi-Supervised* Learning.\n",
    "\n",
    "In the NLP literature, it is called *Unsupervised Learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are abundant sources of raw text data\n",
    "- news, books, blogs, Wikipedia\n",
    "- not all of the same quality\n",
    "\n",
    "The large number of examples that can be generated facilitates the training of models with very large number of weights.\n",
    "\n",
    "This is extremely expensive but, fortunately, the results can be re-used.\n",
    "- Someone with abundant resources trains a Language Model on a broad domain\n",
    "- Publishes the architecture and weights\n",
    "- Others re-use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predict the next ? Really: predict the *distribution* of next\n",
    "\n",
    "We have casually defined the Language Modeling objective as predicting the next token.\n",
    "\n",
    "As you can see: the head layer is a Classifier\n",
    "- produces a probability for *each token* in the vocabulary as being the next\n",
    "$$\n",
    "\\pr{\\y_\\tp \\, | \\, \\y_{(1:\\tt-1)}}\n",
    "$$\n",
    "- We choose one token by sampling from this probability distribution\n",
    "    - Greedy sampling: always chose the token with highest probability\n",
    "    - Non-greedy sampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Masked Language Modeling objective\n",
    "\n",
    "There is a variation on the Language Modeling objective called the  *Masked Language Modeling* objective.\n",
    "- Language Modeling objective: given $s[1:\\tt-1]$, predict $s[\\tt]$\n",
    "- Masked Language Modeling objective\n",
    "    - Given $s[1:\\tt]$\n",
    "    - Randomly chose an index $1 \\le j \\le \\tt$\n",
    "    - \"Mask\" token $j$ by replacing it with `<MASK>` so that the input becomes\n",
    "    $$\n",
    "    s_{(0)},  \\ldots s_{(j-1)}, \\text{<MASK>}, s_{(j+1)}, \\ldots s_\\tp\n",
    "    $$\n",
    "    - Predict the value behing the mask, e.g., $s_{(j)}$\n",
    "- The Language Modeling objective is the special case where $j=\\tt$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Pre-Training + Supervised Fine-Tuning (Transfer Learning)\n",
    "\n",
    "How do we adapt a Language Model to solve other Target tasks ?\n",
    "\n",
    "The obvious answer is via Transfer Learning\n",
    "- The Language Model has learned a lot about the nature of language\n",
    "    - perhaps the language-knowledge can be transfered to a new task\n",
    "- Replace the \"head\" that predicts the next token\n",
    "- With a new task-specific head\n",
    "- Train the new model on labeled examples from the Target task\n",
    "    - the task-specific head **must** be trained\n",
    "    - the language-model weights **can** (but don't have to) be adapted\n",
    "    \n",
    "This paradigm is called *Unsupervised Pre-Traininng + Supervised Fine-Tuning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transfer Learning: replace the head of the pre-trained model</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transfer_Learning_2.jpg\" width=60%></td>\n",
    "    </tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Fine-Tuning a Pre-trained Language Model to analyze sentiment\n",
    "\n",
    "This is a straight-forward application of Transfer Learning\n",
    "- Replace the Classification Head used for Language Modeling\n",
    "    - e.g., a head that generated a probability distribution over words in the vocabulary\n",
    "- By an untrained Binary Classification head (Positive/Negative sentiment)\n",
    "- Train on examples. Pairs of\n",
    "    - sentence\n",
    "    - label: Positive/Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other uses of a Language Model: Feature based Transfer Learning\n",
    "\n",
    "We can generalize the procedure of \"replacing the head\": re-use the features produced by the\n",
    "Source model.\n",
    "\n",
    "Let $f_\\Theta (\\x)$ denote the function computed by the Source Language Model on input sequence $\\x$\n",
    "- the output of the layer **before the final Classification layer** that translates $f_\\Theta (\\x)$ into the token Vocabulary\n",
    "- the Source model is parameterized by $\\Theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Feature based Transfer Learning computes\n",
    "$$\n",
    "g_\\Phi( f_\\Theta (\\x) )\n",
    "$$\n",
    "for the function $g$ (parameterized by $\\Phi$) computed by a new NN.\n",
    "\n",
    "That is\n",
    "- we take the final representation $f_\\Theta (\\x)$ create by the model for the source Task\n",
    "- and feed it into a model for the Target Task\n",
    "-\n",
    "The case where $g_\\Phi$ is a Classifier is the special case of creating a new Target task specific head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using the final representation\n",
    "\n",
    "The final representation of some models may be useful in surprising ways.\n",
    "\n",
    "For example\n",
    "- consider the final representation created by an Encoder Transformer trained on a Language Modeling task\n",
    "- it is a *sequence* (of length $\\bar T$, where the input is also a sequence of length $\\bar T$)\n",
    "    - a \"context sensitive\" representation of each element in the input sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many tasks that use an Encoder modify the original input sequence\n",
    "- by bracketing it with special tokens `<START>, <END>`\n",
    "\n",
    "The context sensitive representation of the `<START>, <END>` tokens\n",
    "- can be interpreted as a **fixed-length summary of the entire input sequence**\n",
    "- similar to the final latent state in an RNN\n",
    "\n",
    "Thus, the Encoder can be used to *summarize a sequence*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Two interesting uses of this fixed-length summary\n",
    "- Classification of sequences: e.g., Sentiment Analysis\n",
    "- Semantic Search\n",
    "    - \"Google\"-like search\n",
    "    - encode your query as a fixed length vector\n",
    "    - encode each document in a collection as a fixed length vector\n",
    "    - -query-retrieval: return the document whose vector is closest to that of the query\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-task learning\n",
    "\n",
    "One area of recent interesting is *multi-task learning*\n",
    "- Training a model to implement multiple tasks\n",
    "\n",
    "A model that implements a single task computes\n",
    "$$\\pr{\\text{output | input}}$$\n",
    "\n",
    "A model that implements several tasks computes\n",
    "$$\\pr{\\text{output | input, task-id }} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using the Universal API, \n",
    "a Language Model may be adapted to solve *multiple tasks* simultaneously.\n",
    "\n",
    "This requires you to construct a training set\n",
    "- with examples from each task\n",
    "- where each example has an additional \"feature\" that identifies the task to which it applies\n",
    "\n",
    "For example\n",
    "\n",
    "$$\\begin{array}[lll] \\\\\n",
    "(  \\mathsf{Translate \\; to \\;French} , & \\text{English text} ,  & & \\text{French Text}) \\\\\n",
    "( \\mathsf{Answer \\; the \\; question} , & \\text{document} , & \\text{question} , & \\text{answer}) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The first feature above is the task identifier of the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
