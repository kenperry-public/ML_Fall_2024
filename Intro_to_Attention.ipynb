{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attention: motivation\n",
    "\n",
    "Let us consider a task familiar to all of us who have taken standardized exams: Question Answering.\n",
    "\n",
    "Input consists of two pieces of text\n",
    "- a paragraph (called the *Context*)\n",
    "- a Question whose answer can be found in the paragraph\n",
    "\n",
    "Output is a piece of text that answers the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\n",
    "\\x = \\;\n",
    "\\begin{Bmatrix}\\\\\n",
    "\\text{Context:} & \\text{The FRE Dept offers many Spring classes.  The students are great. ...} \\\\\n",
    "& \\vdots \\\\\n",
    "& \\text{Professor Perry taught them Machine Learning. The students ...}, \\\\\n",
    "& \\vdots \\\\\n",
    "& \\text{Professor Blecherman led a class in ...} \\\\\n",
    "& \\vdots \\\\\n",
    "\\text{Question:} & \\text{What did Professor Perry do ?} \\\\\n",
    "\\end{Bmatrix}\n",
    "$\n",
    "<br><br><br>\n",
    "$\n",
    "\\y = \\;\n",
    "\\begin{array} \\\\\n",
    "\\text{Answer:} & \\text{He taught them Machine Learning}\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is an example of a *Sequence to Sequence* task\n",
    "- briefly encountered when we defined RNN's\n",
    "\n",
    "Language translation is another common Sequence to Sequence task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us hypothesize how a model might learn to solve this task\n",
    "- it is only an hypothesis: we don't really know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model might have generalized\n",
    "- from seeing many training examples of disparate questions and their answers\n",
    "- that there is a parameterized *template* for both the Question and the Answer\n",
    "\n",
    "Question Template\n",
    "\n",
    "> What did Professor `<PROPER NOUN>` teach in the Spring ?\n",
    "\n",
    "Answer Template:\n",
    "```\n",
    "<PRONOUN> <VERB> <INIDRECT OBJECT> <OBJECT>\n",
    "```\n",
    "\n",
    "where `<PROPER NOUN>, <PRONOUN>, <VERB>`, etc. are *pattern place-holders* parameters.\n",
    "\n",
    "By using a parameterized template, the model captures\n",
    "- commonality\n",
    "- in many different types of questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to produce the answer the model needs to\n",
    "- Generate the tokens of the Answer Template in order\n",
    "- Substituting in concrete values for the place-holders\n",
    "    - by performing a Lookup in the Context in order to obtain these values\n",
    "\n",
    "We will examine how the Lookup might be performed\n",
    "- first: by using mechanisms that we have already studied\n",
    "- subsequently: via a new mechanism called *Attention*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using an RNN without Attention\n",
    "\n",
    "## Encoder-Decoder architecture: review\n",
    "\n",
    "For the Question Answering task\n",
    "- both the Input and Output are sequences\n",
    "- thus, the task is a Sequence to Sequence task\n",
    "    - just like: Language Translation\n",
    "\n",
    "We learned that Recurrent architectures are best-suited for processing sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "These architectures\n",
    "- operate in a \"loop\"\n",
    "    - processing one Input or Output token at a time\n",
    "- utilize **memory** (latent state)\n",
    "    - necessary because Input/Output sequence lengths are unbounded\n",
    "    - after processing the token at position $\\tt$\n",
    "        - the latent state is finite representation of the prefix of the sequence of length $\\tt$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Loop architecture\n",
    "\n",
    "- Uses a \"latent state\" that is updated with each element of the sequence, then predict the output\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\pr{\\h_\\tp | \\x_\\tp, \\h_{(\\tt-1)} } & \\text{latent variable } \\h_\\tp \\text{encodes } [ \\x_{(1)} \\dots \\x_\\tp ]\\\\\n",
    "\\pr{\\hat\\y_\\tp | \\h_\\tp }              & \\text{prediction contingent on latent variable} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "    \n",
    "<br>\n",
    "<div>\n",
    "    <center><strong>Loop with latent state</strong></center>\n",
    "    <img src=\"images/RNN_arch_loop.png\" width=70%>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A common architecture for Sequence to Sequence tasks is the Encoder-Decoder:\n",
    "- The Encoder is an RNN\n",
    "    - Acts on input sequence $[\\x_{(1)} \\dots \\x_{(\\bar{T})}]$\n",
    "    - Producing a sequence of latent states $[ \\bar{\\h}_{(1)}, \\dots, \\bar{\\h}_{(\\bar{T})} ]$\n",
    "        - latent state $\\bar{\\h}_\\tt$ is a summary of $[\\x_{(1)} \\dots \\x_\\tp ]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Decoder\n",
    "- Acts on the *final* Encoder latent state $\\bar{\\h}_{(\\bar{T})}$\n",
    "    - which summarizes the entire input sequence $\\x$\n",
    "    - Producing a sequence of latent states $[ \\h_{(1)}, \\dots, \\h_{(T)} ]$\n",
    "        - latent state $\\h_\\tp$ is response for generating output token $\\hat{\\y}_\\tp$\n",
    "    - Thus outputting a sequence  $[ \\hat{\\y}_{(1)}, \\dots, \\hat{\\y}_{(T)} ]$\n",
    "- Often feeding step $\\tt$ output $\\hat{\\y}_\\tp$ as Encoder input at step $(\\tt+1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Encoder/Decoder</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_Encoder_Decoder.png\"</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decoder output $\\hat\\y_\\tp$\n",
    "\n",
    "The simplest RNN (corresponding to our diagrams) use the latent state $\\h_\\tp$ as the output $\\hat\\y_\\tp$\n",
    "$$\n",
    "\\hat\\y_\\tp = \\h_\\tp\n",
    "$$\n",
    "\n",
    "It is easy to add another NN to transform $\\h_\\tp$ into a $\\hat\\y_\\tp$ that is different.\n",
    "\n",
    "- We can add a NN to the Decoder RNN that implements a function $D$ that transforms the latent state into an output.\n",
    "\n",
    "$$\\hat\\y_\\tp = D(\\h_\\tp)$$\n",
    "\n",
    "For clarity: we will omit this additional NN from our diagrams until it becomes necessary\n",
    "\n",
    "Here is what the additional NN looks like:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Decoder output transformation: No attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Decoder_no_attention.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How does the Decoder perform Lookup (without Attention) ?\n",
    "\n",
    "Suppose the Decoder has already output \n",
    "$$\\hat\\y_{([1:3])} = \\text{He taught them}$$\n",
    "\n",
    "It must subsequently output\n",
    "$$\n",
    "\\hat\\y_{([4:5])} = \\text{Machine Learning}$$\n",
    "\n",
    "In order to do this\n",
    "- it must Lookup \"Machine Learning\" in the Context, resulting in\n",
    "$$\\begin{array} \\\\\n",
    "D( \\h_{(4)}) & = & \\text{Machine} \\\\\n",
    "D( \\h_{(5)}) & = & \\text{Learning} \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But  $D$ is conditioned on the single input $\\h_\\tp$.\n",
    "\n",
    "Thus, in order for $D( \\h_{(4)} )$ to be equal to \"Machine\"\n",
    "- this information must somehow be encoded in $\\h_{(4)}$\n",
    "\n",
    "\n",
    "How did it get there ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "All \"knowledge\" from the Context must be transfered from Encoder to Decoder\n",
    "- through final Encoder state $\\bar \\h_{(\\bar T)}$\n",
    "- which in turn was encoded in all Encoder states $\\bar \\h_{({\\bar \\tt}')} \\text{ for } \\tt' \\ge \\bar p$\n",
    "    - where $\\bar p$ is the position withing sequence $\\x$ of the word \"Machine\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can hypothesize that the final Encoder latent state $\\bar\\h_{\\bar T}$\n",
    "- encodes a Dictionary (key/value pairs)\n",
    "- mapping Place-holder names to Concrete values\n",
    "- the dictionary is built incrementally by prior latent states of the Encoder\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Answering questions using Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_example_1.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This dictionary is passed to the Decoder via the single connection from Encoder to Decoder\n",
    "- and must be carried forward by the Decoder\n",
    "- through Decoder states $[ \\h_{(1)}, \\dots, \\h_{(4)} ]$\n",
    "- in order to make the dictionary available to subsequent latent states of the Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We further hypothesize that the Decoder\n",
    "- performs Lookups \n",
    "- by using the Decoder latent state $\\h_\\tp$\n",
    "    - as a *query* that matches against the keys of the Dictionary\n",
    "    - in order to obtain the Concrete value required to produced output token at position $\\tt$\n",
    "    $$\\hat\\y_\\tp = D(\\h_\\tp)$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Query performing a Lookup in the Dictionary</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_example_2.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a picture describing this hypothetical functioning.\n",
    "<br>\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Encoder/Decoder without Attention<br>Bottleneck</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_Encoder_Decoder_1.png\" width=80%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Encoder_Decoder_no_attention.png\" width=70%</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Connecting the Encoder and Decoder through the \"bottleneck\" of $\\bar \\h_{(\\bar T)}$ thus burdens the\n",
    "- Encoder: passing knowledge forward to the bottleneck\n",
    "- Decoder: passing knowledge from the bottleneck\n",
    "    - to all subsequent positions (red box within each circle)\n",
    "\n",
    "This results in an inefficient use of the model's latent state variable\n",
    "- In addition to\n",
    "    - the Encoder and Decoder allocating some of the model's latent state for \"control\"\n",
    "    - guiding the loop that processes the Input, or generates the output positions in the template\n",
    "- It must **also** allocate some of the model's latent state for \"knowledge storage\"\n",
    "    - in order to Lookup the concrete value corresponding to a place-holder in the Output template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cross Attention\n",
    "\n",
    "**Reference**\n",
    "\n",
    "[Neural Machine Translation by Jointly Learning To Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "[paper that introduced Attention](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "\n",
    "<br><br>\n",
    "The flaw in the Encoder-Decoder without Attention is \n",
    "- the input $\\x$ is processed *only once*\n",
    "- by the Encoder\n",
    "- which has to summarize it in $\\bar{\\h}_{(\\bar T)}$\n",
    "\n",
    "We will introduce a mechanism called *Attention*\n",
    "- that allows the input sequence to be *re-visited* at each time step of Output generation\n",
    "\n",
    "This will result in a cleaner separation between control memory and input memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Attention allows the Decoder\n",
    "- to directly access all of the Encoder latent states $\\bar\\h_{(1)} \\dots \\h_{(\\bar{T})}$\n",
    "- at each time step of the Decoder\n",
    "\n",
    "Thus, there is no need\n",
    "- for an Encoder to create a full dictionary as the final Encoder latent state $\\bar\\h_{(\\bar T)}$\n",
    "- for the Decoder to keep the dictionary in all it's latent states $\\h_{(1)} \\dots \\h_{(T)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a picture of an Encoder/Decoder augmented with Attention\n",
    "- we have add an additional box to the diagram for the NN that implements the function $D$\n",
    "    - that maps $\\h_\\tp$ to $\\y_\\tp$\n",
    "    \n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Encoder/Decoder with Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_Encoder_Decoder_Attention_1.png\" width=80%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Encoder_Decoder_no_attention.png\" width=70%</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is one \"flavor\" of Attention known as *Cross Attention*\n",
    "- one component (the Decoder) uses as input (*attends to*) the output of another component (the Encoder)\n",
    "\n",
    "Decoder has *direct access* to **all** outputs (i.e., Latent sates) of the Encoder\n",
    "- each Encoder output is proxy for a prefix of the input\n",
    "    \n",
    "The pink box is the sequent of Encoder outputs\n",
    "$$\n",
    "\\bar\\h_{(1:\\bar T)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice that the final Encoder latent state $\\bar\\h_{(\\bar T)}$ is **no longer**  connected to the Decoder.\n",
    "\n",
    "What is going on inside the \"box\" implementing function $D$ that we added at each time step ?\n",
    "\n",
    "The box's input at step $\\tt$\n",
    "- the Decoder latent state $\\h_\\tp$\n",
    "- the collection of Encoder latent states $\\bar\\h_{(1)} \\dots \\h_{(\\bar{T})}$\n",
    "    - the red box in the above diagram\n",
    "\n",
    "That is, it is computing a $\\hat\\y_\\tp$ that is a function of both $\\h_\\tp$ and $\\bar\\h_{(1)} \\dots \\h_{(\\bar{T})}$\n",
    "\n",
    "$$\n",
    "\\hat\\y_\\tp = D( \\h_\\tp,  [ \\bar\\h_{(1)} \\dots \\h_{(\\bar{T})} ])\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Self-Attention: removing the Encoder loop\n",
    "\n",
    "There is an alternative to the loop architecture for processing sequences\n",
    "- the direct function approach\n",
    "\n",
    "The alternative to the loop was to create a \"direct function\"\n",
    "- Taking a **sequence** $\\x_{(1 \\dots \\tt)}$ as input\n",
    "- Outputting $\\hat\\y_\\tp$\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "    <center><strong>Direct function</strong></center>\n",
    "    <img src=\"images/RNN_arch_parallel.png\" width=50%>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that output at position $\\tt$ *no longer depend* on latent state from position $(\\tt-1)$\n",
    "- we have removed the loop !\n",
    "\n",
    "Thus, the Encoder can output *all* elements of sequence $\\bar \\h$ *simultaneously*\n",
    "- n.b. the Encoder output sequence will still be denoted\n",
    "$$\\bar\\h_{(1:\\bar T)}$$\n",
    "\n",
    "even though the outputs are no longer *latent states*\n",
    "\n",
    "- each output position is independent of previous output\n",
    "- only dependent on input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We removed the \"loop\" architecture of the Encoder by using  the direct function approach\n",
    "- the mechanism enabling each position of the Encoder output to *attend* to the entire sequence $x$ is called *Self-Attention*\n",
    "    - Notice: no dependency arrow between circles in the Encoder\n",
    "- Encoder output is a direct function of **all** positions in the input\n",
    "    - all Encoder output positions can be computed *in parallel*\n",
    "\n",
    "The blue box represents the *entire* input sequence\n",
    "$$\n",
    "\\x_{(1:\\bar T)}\n",
    "$$\n",
    "We no longer refer to the Encoder output as a Latent state\n",
    "- no more loop !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Encoder/Decoder with Cross Attention/Decoder Self Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_Encoder_Decoder_Attention_Encoder_Self_Attention.png\"\n",
    "             width=80%</td>\n",
    "    </tr>\n",
    "   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Masked Self Attention: removing the Decoder loop\n",
    "\n",
    "Finally we remove the loop architecture for the Decoder as well using\n",
    "a different \"flavor\" of Self-Attention\n",
    "\n",
    "The grey box represents the *entire* output sequence\n",
    "$$\n",
    "\\hat\\y_{(1:T)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Encoder/Decoder with Cross Attention and Self Attention (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_Encoder_Decoder_Attention_All_Self_Attention.png\"\n",
    "             width=80%</td>\n",
    "    </tr>\n",
    "   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now\n",
    "- the output sequence $\\hat\\y$ is built iteratively (auto-regressively)\n",
    "- units work in parallel\n",
    "- each iteration outputs *all* positions\n",
    "$$\n",
    "\\hat\\y_{(1:T)}\n",
    "$$\n",
    "    - including ones whose full inputs have not been defined yet!\n",
    "    - $\\hat\\y_\\tp$ is not defined until iteration $\\tt$\n",
    "\n",
    "This is confusing !\n",
    "\n",
    "The point is we don't output position $\\tt$ to the user until iteration $\\tt$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We certainly don't want $\\hat\\y_\\tp$ to change\n",
    "on iterations $\\tt' \\gt \\tt$\n",
    "- don't want future outputs $\\hat\\y_{(\\tt')}$ for $\\tt' \\ge \\tt$ to affect $\\hat\\y_\\tp$ \n",
    "- $\\hat\\y_\\tp$ depends *only* on $\\hat\\y_{(1:\\tt-1)}$\n",
    "\n",
    "We can ensure this by using **Masked Self Attention**\n",
    "- position $\\tt$ can only access positions $\\tt' \\lt \\tt$\n",
    "$$\n",
    "\\hat\\y_{(1:\\tt-1)}\n",
    "$$\n",
    "\n",
    "This means that outputs after iteration $\\tt$ *can't effect* $\\hat\\y_\\tp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visualizing Attention\n",
    "\n",
    "We can illustrate the behavior of Neural Networks that have been augmented with Attention through diagrams.\n",
    "- at a particular output position $\\tt$\n",
    "- we can display the amount of \"attention\"\n",
    "- that each position in the input receives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing Self-Attention\n",
    "\n",
    "Self Attention can be used to create a Context Sensitive Encoding of words\n",
    "- The meaning of a word may change depending on the rest of the sentence\n",
    "\n",
    "We can illustrate this with an example: how the meaning of the word \"it\" changes\n",
    "- The thickness of the blue line indicates the attention weight that is given in processing the word \"it\".\n",
    "\n",
    "<img src=https://1.bp.blogspot.com/-AVGK0ApREtk/WaiAuzddKVI/AAAAAAAAB_A/WPV5ropBU-cxrcMpqJBFHg73K9NX4vywwCLcBGAs/s1600/image2.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Much of the recent advances in NLP may be attributed to these improved, context sensitive embeddings.\n",
    "\n",
    "We note that simple Word Embeddings\n",
    "- also capture \"meaning\"\n",
    "- but are *not* sensitive to context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing Cross Attention\n",
    "\n",
    "## The Entailment task\n",
    "\n",
    "The *Entailment* task is a binary classification task based on two sentence\n",
    "- first sentence (the *premise*)\n",
    "- second sentence (the *hypothesis*)\n",
    "\n",
    "Classify: \n",
    "Does the Hypothesis logically follow (is *entailed* by) the Premise ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is an illustration of what part of the Premise is attended to\n",
    "as we encounter the Hypothesis\n",
    "\n",
    "<br>\n",
    "<center><strong>Attention: Entailment</strong></center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_visualization_Entailment.png\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>Does the Premise logically entail the Hypothesis.</center></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Attribution: https://arxiv.org/pdf/1509.06664.pdf#page=6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Date Normalization task\n",
    "\n",
    "- Source: Dates in free-form: \"Saturday 09 May 2018\"\n",
    "- Target: Dates in normalized form: \"2018-05-09\"\n",
    "\n",
    "[link](https://github.com/datalogue/keras-attention#example-visualizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The  Image captioning task\n",
    "\n",
    "- Source: Image\n",
    "- Target: Caption: \"A woman is throwing a **frisbee** in a park.\"\n",
    "- Attending over *pixels* **not** sequence\n",
    "\n",
    "<center><strong>Visual attention</strong></center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/shat_-002-027.jpg\"></td>\n",
    "        <td><img src=\"images/shat_-002-028.jpg\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=2><center>A woman is throwing a <strong>frisbee</strong> in a park.</center></td>\n",
    "    </tr>\n",
    "</table>\n",
    "Attribution: https://arxiv.org/pdf/1502.03044.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementing Attention (Preview): Inside the NN for $D$\n",
    "\n",
    "Recall the Decoder conditions output $\\hat\\y_\\tp$ on\n",
    "- Decoder state $\\h_\\tp$\n",
    "- Encoder output sequence $\\bar \\h_{(1:\\bar T)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a high-level view of the inner workings of the NN for $D$:\n",
    "    \n",
    "<br>\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Decoder output transformation with attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Decoder_attention.png\" width=60%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Inside the box:\n",
    "- the Decoder latent state $\\h_\\tp$ is used as a *query*\n",
    "- which is matched against each of the Encoder outputs\n",
    "- resulting in one Encoder output being chosen as $\\mathbf{c}_\\tp$\n",
    "\n",
    "The chosen Encoder output $\\mathbf{c}_\\tp$ and Decoder latent state $\\h_\\tp$\n",
    "- are input to another Neural Network\n",
    "- which produces output $\\hat\\y_\\tp$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "The \"Choose\" box implements an *Attention* mechanism, which allows the Decoder\n",
    "- to **attend to** the part of Input $\\x$ (represented via some Encoder latent state $\\bar\\h_{(\\bar\\tt)}$)\n",
    "- that is *relevant* for producing $\\hat \\y_\\tp$\n",
    "- exactly when it is needed\n",
    "\n",
    "This seems very natural to a human\n",
    "- rather than memorizing details (e.g., the big dictionary $\\bar\\h_{(\\bar T)}$ in the architecture without Attention)\n",
    "- we refer back to the context\n",
    "- focusing of only the part that is immediately needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The discussion of the **implementation** of Attention will be deferred to a\n",
    "later module [Attention lookup](Attention_Lookup.ipynb).\n",
    "\n",
    "For now, think of the \"Choose\" box as a Context Sensitive Memory (as described in the module on [Neural Programming](Neural_Programming.ipynb#Soft-Lookup))\n",
    "- Like a Python `dict`\n",
    "    - Collection of key/value pairs: $\\langle \\bar\\h_{(\\bar \\tt)}, \\bar\\h_{(\\bar \\tt)} \\rangle$\n",
    "    - Key is equal to value; they are latent states of the Encoder\n",
    "- But with *soft* lookup\n",
    "    - The current Decoder state $\\h_\\tp$ is presented to the CSM \n",
    "        - Called the *query*\n",
    "        - Is matched across each key of the dict (i.e., a latent state $\\bar \\h_{(\\bar \\tt)}$)\n",
    "    - The CSM returns an approximate match of the query to a *key* of the `dict`\n",
    "        - The distance between the query and each key in the CSM is computed\n",
    "        - The Soft Lookup returns a *weighted* (by inverse distance) sum of the *values* in the CSM `dict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-head attention: two heads are better than one\n",
    "\n",
    "Perhaps when generating the output for position $\\tt$ of the output sequence\n",
    "- we need to attend to *more than one* position of the sequence being attended to\n",
    "    - need to know both gender and plurality of subject\n",
    "- that is: we want an Attention layer to output multiple items.\n",
    "\n",
    "We can attend to $n$ positions\n",
    "- by creating $n$ separate Attention mechanisms\n",
    "- each one called a *head*\n",
    "\n",
    "This behavior is referred to as *Multi-head attention*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This type of behavior is common to many layer types in a Neural Network\n",
    "- a Dense layer $l$ may produce a vector $\\y_\\llp$ where $n_\\llp \\gt 1$\n",
    "- a Convolutional layer $l$ may produce outputs (for each spatial location) for many channels\n",
    "\n",
    "We have referred to this as layer $\\ll$ producing $n_\\llp$ *features*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It would be natural for an Attention layer to output many \"features\" to enable attention to many positions.\n",
    "\n",
    "In practice, this is sometimes (always ?) not done\n",
    "- Model architectures (e.g., the Transformer) are simplified when the inputs/outputs of each sub-component\n",
    "- have the same length\n",
    "- often denoted as $d$ or $d_\\text{model}$ in the Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When a Transformer needs to attend to $n$ positions\n",
    "- it uses $n$ Attention heads\n",
    "- each outputting a vector of length $\\frac{d}{n}$\n",
    "- which are concatenated together to produce a single output of length $d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In essence\n",
    "- we create $n$ \"mini-attention\" heads\n",
    "    - we uses queries\n",
    "    - and input sequences\n",
    "    -which are a *fraction* of the original lengths\n",
    "- the outputs of the $n$ mini-heads can be combined into an output of length $d$\n",
    "\n",
    "So multi-head attention is compatible (in terms of shape of input and output) with\n",
    "single head attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When we have $n$ heads\n",
    "- Rather than having one Attention head operating on vectors of length $d$\n",
    "    - producing an output of length $d$ (weighted sum of values in the CSM)\n",
    "- We create $n$ Attention heads operating on vectors (keys, values, queries) of length $d \\over n$.\n",
    "    - Output of these smaller heads are values, and hence also of length $d \\over n$\n",
    "- The final output concatenates these $n$ outputs into a single output of length $d$\n",
    "    - identical in length to the single head\n",
    "- we project each of these length $d$ vector into vectors of length $d \\over n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The picture shows $n$ Attention heads.\n",
    "\n",
    "Note that each head is working on vectors of length $\\frac{d}{n}$ rather than\n",
    "original dimensions $d$.\n",
    "- variables with superscript $(j)$ are of fractional length\n",
    "\n",
    "Details are deferred to the module [Attention lookup](Attention_Lookup.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each head $j$ uniquely transforms the query $\\h_\\tp$ and the key/value pairs $\\bar{\\h}_{(1)} \\ldots \\bar{\\h}_{(\\bar{T})}$ being queried.\n",
    "- into $\\h^{(j)}_\\tp$ and the key/value pairs $\\bar{\\h}^{(j)}_{(1)} \\ldots \\bar{\\h}^{(j)}_{(\\bar{T})}$\n",
    "- all vectors/vector elements with superscripts are of length $\\frac{d}{n}$ rather than the original length $d$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Decoder Multi-head Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Multihead_attention.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Teacher forcing: another use case for Masked Self Attention\n",
    "\n",
    "You may wonder how it is even practically possible for a Decoder to refer to the future.\n",
    "\n",
    "When using *Teacher Forcing* for **training**\n",
    "- the Decoder does not use the *predicted* target sequence $\\hat \\y_{(1:T)}$\n",
    "- the Decoder uses the *actual* target sequence $\\y_{(1:T)}$\n",
    "    - hence, \"future\" positions $\\tt' \\ge \\tt$ are available\n",
    "- this prevents a single mis-prediction at position $\\tt$ from cascading and ruining all future output\n",
    "    - facilitates training\n",
    "- at inference time: the Decoder works on the *predicted* Target sequence.\n",
    "\n",
    "In the diagram below, we illustrate (lower right) how the Decoder input changes between Training and Test/Inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Sequence to Sequence: training (teacher forcing) + inference: No attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_seq2seq.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformers (preview)\n",
    "\n",
    "Using various flavor of Attention\n",
    "- we have replaced the Encoder and Decoder RNN's in an Encoder/Decoder architecture\n",
    "- with a new layer type\n",
    "    - direct function approach to sequences\n",
    "    \n",
    "This new architecture is the basis for the *Transformer* layer\n",
    "- a key advance in modern Deep Learning, particularly for NLP\n",
    "- which we will study in more depth in a subsequent module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "We recognized that the Decoder function responsible for generating Decoder output $\\hat{\\y}_\\tp$\n",
    "$$\n",
    "\\hat{\\y}_\\tp = D( \\h_\\tp; \\mathbf{s})\n",
    "$$\n",
    "\n",
    "was quite rigid when it ignored argument $\\mathbf{s}$.\n",
    "\n",
    "This rigidity forced Decoder latent state $\\h_\\tp$ to assume the additional responsibility of including Encoder context.\n",
    "\n",
    "Attention was presented as a way to obtain Encoder context through argument $\\mathbf{s}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.583px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
